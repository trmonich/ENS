{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# CFM Challenge\n",
    "2018-07. \n",
    "\n",
    "_Note_ : This model gives lower scores than the actual results on the challenge : \n",
    "   - Indeed the notebook will give you around ~21.35 by averaging the 5kfolds which will give around ~20.94 on the public leaderboard. This might be due to high regularisation on the model and also because the test set give better predictions than the train set. \n",
    "\n",
    "To get the best score  on the leaderboard at the end of the competition (around ~20.88), I've just averaged the results of the top 10 best csvs results with different models I've tried during the competion.\n",
    "The different models tried were always pretty the sames. What basically changes is the size of the layers, the concatenation/multiplication/addition in the model, the way you deal with nans in the dataset and what kind of engineered features you will take in input + embeddings size (ok that's a lot of changes... but no matter what you do, you will have more or else the same result with this model).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "If you want more details about the model and how does it work, you can find it on the attached slides. This presentation was given at CFM and the end of the academic competition and gives some insights of what worked and what did not.\n",
    "\n",
    "PS : The script isn't really well coded...  basically because I didn't really take the time to do a whole refactoring and it was made of a lot of iterations. So some stuff should not be done like this anymore and doesn't really make sens.. (sorry for that !) \n",
    "\n",
    "If you have any question or suggestions (or if you get way better scores than me !), let me know, I would be happy to discuss and learn ;).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's import some stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import multiply, Layer, LeakyReLU, PReLU, Bidirectional, merge, Dense, GlobalAveragePooling1D, SpatialDropout1D,concatenate,add,Embedding,Input,Masking,LSTM,Flatten,TimeDistributed,GlobalMaxPooling1D,GRU,Activation,Dropout,Reshape, BatchNormalization\n",
    "from sklearn.preprocessing import LabelEncoder, Imputer, StandardScaler, RobustScaler, MinMaxScaler\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau,TensorBoard, LearningRateScheduler\n",
    "from keras import losses\n",
    "from keras import initializers, regularizers, constraints\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import KFold,train_test_split\n",
    "import keras\n",
    "import hashlib\n",
    "import math\n",
    "import h5py\n",
    "\n",
    "\n",
    "#These libs are not from me, I've just copy past the code to use them... \n",
    "#You can find the original repos below and the papers associated in the code.\n",
    "from CLR.clr_callback import CyclicLR #https://github.com/bckenstler/CLR/blob/master/clr_callback.py | https://arxiv.org/abs/1506.01186\n",
    "from JANET.janet import JANET #https://github.com/titu1994/Keras-just-another-network-JANET/blob/master/janet.py | https://arxiv.org/abs/1804.04849\n",
    "import gc\n",
    "print(keras.__version__)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=Warning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_dir = \"../data/\"\n",
    "\n",
    "id_col = \"ID\"\n",
    "target_col = \"TARGET\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     eqt_code        date  09:30:00  09:35:00  09:40:00  09:45:00  09:50:00  \\\n",
      "ID                                                                            \n",
      "0   975514820  2059277756  0.378421  0.062977 -1.699955 -0.253532  0.317230   \n",
      "1   975514820   661006643 -0.588441 -0.393182  0.918810  0.392393  0.195901   \n",
      "2   975514820  2252275018 -0.974472  1.789703 -0.105310 -0.456042  0.894337   \n",
      "3   975514820   901241465 -4.612982  1.746131  1.092367 -0.030626  0.996415   \n",
      "4   975514820  1213815424 -2.642804 -0.772708 -0.115318  0.145272 -0.136105   \n",
      "\n",
      "    09:55:00  10:00:00  10:05:00  ...  14:40:00  14:45:00  14:50:00  14:55:00  \\\n",
      "ID                                ...                                           \n",
      "0  -1.394081 -1.338014 -0.320252  ... -0.193950  0.129398  0.064666  0.323250   \n",
      "1  -0.195754  0.000000  0.130601  ... -0.260940  0.000000 -0.130601  0.130666   \n",
      "2  -0.341168 -0.373679  1.103954  ... -0.236124 -0.256959 -0.229774  0.063974   \n",
      "3  -1.391209  0.169799 -0.827777  ... -0.185523  0.521719 -0.095356 -0.207925   \n",
      "4   0.450894 -0.046230  1.186248  ...  0.090185 -0.169416 -0.081212 -0.275152   \n",
      "\n",
      "    15:00:00  15:05:00  15:10:00  15:15:00  15:20:00  end_of_day_return  \n",
      "ID                                                                       \n",
      "0   0.193705  0.387117  0.257687  0.064357 -0.193022           0.964025  \n",
      "1   0.130601  0.130536  0.521881  0.260419  0.910557          -0.583286  \n",
      "2  -0.570371  0.133774 -0.123741 -0.764930  0.230346           2.191065  \n",
      "3   0.318842  0.119798 -0.155044  0.290896 -0.058833           0.406176  \n",
      "4  -0.249646  0.042280  0.483204  0.394914 -0.619209          -0.053764  \n",
      "\n",
      "[5 rows x 74 columns]\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"%s/input_training.csv\" % home_dir)\n",
    "id_df = pd.read_csv(\"%s/output_training_hTMcdrS.csv\" % home_dir)\n",
    "test_df = pd.read_csv(\"%s/input_test.csv\" % home_dir)\n",
    "train_df  = train_df.merge(id_df, on=\"ID\")\n",
    "train_df = train_df.set_index('ID')\n",
    "del id_df; gc.collect()\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.rename(index = str, columns = {'end_of_day_return' : 'TARGET'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['09:30:00' '09:35:00' '09:40:00' '09:45:00' '09:50:00' '09:55:00'\n",
      " '10:00:00' '10:05:00' '10:10:00' '10:15:00' '10:20:00' '10:25:00'\n",
      " '10:30:00' '10:35:00' '10:40:00' '10:45:00' '10:50:00' '10:55:00'\n",
      " '11:00:00' '11:05:00' '11:10:00' '11:15:00' '11:20:00' '11:25:00'\n",
      " '11:30:00' '11:35:00' '11:40:00' '11:45:00' '11:50:00' '11:55:00'\n",
      " '12:00:00' '12:05:00' '12:10:00' '12:15:00' '12:20:00' '12:25:00'\n",
      " '12:30:00' '12:35:00' '12:40:00' '12:45:00' '12:50:00' '12:55:00'\n",
      " '13:00:00' '13:05:00' '13:10:00' '13:15:00' '13:20:00' '13:25:00'\n",
      " '13:30:00' '13:35:00' '13:40:00' '13:45:00' '13:50:00' '13:55:00'\n",
      " '14:00:00' '14:05:00' '14:10:00' '14:15:00' '14:20:00' '14:25:00'\n",
      " '14:30:00' '14:35:00' '14:40:00' '14:45:00' '14:50:00' '14:55:00'\n",
      " '15:00:00' '15:05:00' '15:10:00' '15:15:00' '15:20:00']\n"
     ]
    }
   ],
   "source": [
    "return_cols = train_df.columns.values[2:73]\n",
    "other_cols = [\"date\" , \"eqt_code\"]\n",
    "print(return_cols)\n",
    "#del train_df; gc.collect()\n",
    "#del test_df; gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sum of nans\n",
    "for df in [train_df,test_df]:\n",
    "    df[\"ret_nan\"] = df[return_cols].isnull().sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace nans by interpolation\n",
    "for df in [train_df,test_df]:\n",
    "    df[return_cols] = df[return_cols].interpolate(axis=1, limit_direction=\"both\", inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_groupby(all_data, groupby_cols):\n",
    "    for groupby_obj in groupby_cols:\n",
    "        groupby_col = groupby_obj[\"id\"]\n",
    "        print(groupby_col)\n",
    "        cols = groupby_obj[\"cols\"]\n",
    "        group_by = all_data.groupby([groupby_col])\n",
    "        data_arr = []\n",
    "        data_arr.append({\"i\": \"avg\", \"d\": group_by[cols].mean()})\n",
    "        data_arr.append({\"i\": \"skew\", \"d\": group_by[cols].skew()})\n",
    "        data_arr.append({\"i\": \"kurt\", \"d\": group_by[cols].apply(pd.DataFrame.kurt)})\n",
    "        data_arr.append({\"i\": \"std\", \"d\": group_by[cols].std()})\n",
    "        data_arr.append({\"i\": \"median\", \"d\": group_by[cols].median()})\n",
    "        data_arr.append({\"i\": \"nan\", \"d\": all_data.isnull().groupby(all_data[groupby_col])[cols].sum()})\n",
    "\n",
    "        all_data.set_index([groupby_col], inplace=True)\n",
    "        for obj_data in data_arr:\n",
    "            names = ['%s_%s_%s' % (obj_data[\"i\"], groupby_col, col) for col in cols]            \n",
    "            all_data[names] = (obj_data[\"d\"]).astype(\"float32\")\n",
    "        all_data.reset_index(inplace=True)\n",
    "    return all_data\n",
    "\n",
    "def group_by_date_countd(all_data):\n",
    "    groupby_col = \"date\"\n",
    "    unique_products = all_data.groupby([groupby_col])[\"eqt_code\"].nunique()\n",
    "    all_data.set_index([groupby_col], inplace=True)\n",
    "    all_data[\"countd_product\"] = unique_products.astype('uint16')\n",
    "    all_data.reset_index(inplace=True)\n",
    "    return all_data\n",
    "\n",
    "\n",
    "def group_by_product_countd(all_data):\n",
    "    groupby_col = \"eqt_code\"\n",
    "    unique_date = all_data.groupby([groupby_col])[\"date\"].nunique()\n",
    "\n",
    "    all_data.set_index([groupby_col], inplace=True)\n",
    "    all_data[\"countd_date\"] = unique_date.astype('uint16')\n",
    "    all_data.reset_index(inplace=True)\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(all_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new aggregated features\n",
      "date\n",
      "eqt_code\n"
     ]
    }
   ],
   "source": [
    "#group by date and product_id to get more features\n",
    "calculation_group_by =[\n",
    "    {\"id\":\"date\" ,\n",
    "     \"cols\": return_cols,\n",
    "    },\n",
    "    {\"id\":\"eqt_code\" ,\n",
    "     \"cols\": return_cols,\n",
    "    }\n",
    "]\n",
    "train_df[\"is_train\"] = True\n",
    "test_df[\"is_train\"] = False\n",
    "test_df[\"TARGET\"] = None\n",
    "all_data = pd.concat([train_df, test_df])\n",
    "del train_df\n",
    "del test_df\n",
    "\n",
    "print(\"Creating new aggregated features\")\n",
    "all_data = get_mean_groupby(all_data, calculation_group_by)\n",
    "print(\"Group by date\")\n",
    "all_data = group_by_date_countd(all_data)\n",
    "print(\"Group by product\")\n",
    "all_data = group_by_product_countd(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace nan per 0\n",
    "rep_values= 0\n",
    "all_data = all_data.replace([np.inf, -np.inf], rep_values)\n",
    "all_data.fillna(rep_values, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_name = \"all_data.pkl\"\n",
    "#all_data.to_pickle(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare to Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.read_pickle(\"all_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['09:30:00' '09:35:00' '09:40:00' '09:45:00' '09:50:00' '09:55:00'\n",
      " '10:00:00' '10:05:00' '10:10:00' '10:15:00' '10:20:00' '10:25:00'\n",
      " '10:30:00' '10:35:00' '10:40:00' '10:45:00' '10:50:00' '10:55:00'\n",
      " '11:00:00' '11:05:00' '11:10:00' '11:15:00' '11:20:00' '11:25:00'\n",
      " '11:30:00' '11:35:00' '11:40:00' '11:45:00' '11:50:00' '11:55:00'\n",
      " '12:00:00' '12:05:00' '12:10:00' '12:15:00' '12:20:00' '12:25:00'\n",
      " '12:30:00' '12:35:00' '12:40:00' '12:45:00' '12:50:00' '12:55:00'\n",
      " '13:00:00' '13:05:00' '13:10:00' '13:15:00' '13:20:00' '13:25:00'\n",
      " '13:30:00' '13:35:00' '13:40:00' '13:45:00' '13:50:00' '13:55:00'\n",
      " '14:00:00' '14:05:00' '14:10:00' '14:15:00' '14:20:00' '14:25:00'\n",
      " '14:30:00' '14:35:00' '14:40:00' '14:45:00' '14:50:00' '14:55:00'\n",
      " '15:00:00' '15:05:00' '15:10:00' '15:15:00' '15:20:00']\n"
     ]
    }
   ],
   "source": [
    "return_cols = all_data.columns.values[2:73]\n",
    "other_cols = [\"date\" , \"eqt_code\"]\n",
    "print(return_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all cols per type\n",
    "#TODO refactor to do it in a for loop \n",
    "\n",
    "return_avg_date_cols = [\"avg_date_%s\" % x for x in return_cols]\n",
    "return_avg_product_cols = [\"avg_eqt_code_%s\" % x for x in return_cols]\n",
    "\n",
    "return_std_date_cols = [\"std_date_%s\" % x for x in return_cols]\n",
    "return_std_product_cols = [\"std_eqt_code_%s\" % x for x in return_cols]\n",
    "\n",
    "return_median_date_cols = [\"median_date_%s\" % x for x in return_cols]\n",
    "return_median_product_cols = [\"median_eqt_code_%s\" % x for x in return_cols]\n",
    "\n",
    "return_kurt_date_cols = [\"kurt_date_%s\" % x for x in return_cols]\n",
    "return_kurt_product_cols = [\"kurt_eqt_code_%s\" % x for x in return_cols]\n",
    "\n",
    "return_nan_date_cols = [\"nan_date_%s\" % x for x in return_cols]\n",
    "return_nan_product_cols = [\"nan_eqt_code_%s\" % x for x in return_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode and preprocess data\n",
    "\n",
    "def fit_encoder(obj_in_cols_obj, all_data, encoder_name=\"StandardScaler\"):\n",
    "    if encoder_name is not None:\n",
    "        for i,x in enumerate(obj_in_cols_obj):\n",
    "            group_cols = x[\"cols\"] #group of lstm cols\n",
    "            encoders = []\n",
    "            for cols in group_cols:\n",
    "                X_all = all_data[cols].values\n",
    "                encoder = df_mapper(cols, X_all, encoder_name)\n",
    "                encoders.append(encoder)\n",
    "            obj_in_cols_obj[i][\"encoders\"] = encoders\n",
    "    return obj_in_cols_obj\n",
    "\n",
    "def df_mapper(cols, all_data , encoder):\n",
    "    if encoder == \"StandardScaler\":\n",
    "        encoder = StandardScaler() \n",
    "    elif encoder == \"LabelEncoder\":\n",
    "        encoder = LabelEncoder()\n",
    "    elif encoder == \"RobustScaler\":\n",
    "        encoder = RobustScaler()\n",
    "    elif encoder == \"MinMaxScaler\":\n",
    "        encoder = MinMaxScaler(feature_range=(0,1))\n",
    "    else:\n",
    "        raise NotFoundException()\n",
    "        \n",
    "    encoder = encoder.fit(all_data)\n",
    "    return encoder\n",
    " \n",
    "def prepare_data(lstm_in_cols_obj, emb_in_cols_obj, df_to_prepare):\n",
    "    def __prepare_data_inn(cols, df_tmp,  encoders, encode=True, reshape=True):\n",
    "        Xs = []\n",
    "        for i, col in enumerate(cols):\n",
    "            X = df_tmp[col].values # timeseries cols\n",
    "            if encode and encoders[i] is not None:\n",
    "                X = encoders[i].transform(X)\n",
    "            if reshape:\n",
    "                X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "            Xs.append(X)\n",
    "        X = np.concatenate([x for x in Xs] , axis=len(Xs[0].shape)-1) if len(Xs) > 1 else Xs[0] \n",
    "        return X\n",
    "       \n",
    "    Xs_map = []\n",
    "    for x in lstm_in_cols_obj:\n",
    "        cols = x[\"cols\"]\n",
    "        encoders = x[\"encoders\"] if \"encoders\" in x else None\n",
    "        encode = True if \"encoders\" in x else False\n",
    "        Xs_map.append(__prepare_data_inn(cols, df_tmp=df_to_prepare, reshape=True, encoders=encoders, encode=encode))\n",
    "    #TODO Refactor ravel\n",
    "    embs = []\n",
    "    for x in emb_in_cols_obj:\n",
    "        cols = x[\"cols\"]\n",
    "        encoders = x[\"encoders\"] if \"encoders\" in x else None\n",
    "        encode = True if \"encoders\" in x else False\n",
    "        X_emb = __prepare_data_inn(cols, df_tmp=df_to_prepare, reshape=False, encoders=encoders, encode=encode)\n",
    "        embs.append(X_emb)\n",
    "    X_t = [x for x in Xs_map] + embs\n",
    "\n",
    "    return X_t\n",
    "\n",
    "\n",
    "def generate(df_generator, target, batch_size, lstm_in_cols_obj,emb_in_cols_obj, y_vals=True, shuffle=True):\n",
    "    imax = int(math.ceil((df_generator.shape[0])/batch_size))\n",
    "    while True:\n",
    "        indexes = __get_exploration_order(df_generator, shuffle)\n",
    "        for i in range(imax):\n",
    "            upp = min((i+1)*batch_size, df_generator.shape[0])\n",
    "            df_tmp = df_generator.loc[indexes[i*batch_size:upp]]\n",
    "            X = prepare_data(lstm_in_cols_obj,emb_in_cols_obj, df_tmp)\n",
    "            if y_vals:\n",
    "                y = (df_tmp[target].values >= 0)\n",
    "                yield X, y\n",
    "            else:\n",
    "                yield X\n",
    "\n",
    "                \n",
    "def __get_exploration_order(df, shuffle=True):\n",
    "    indexes = df.index.values\n",
    "    if shuffle == True:\n",
    "        np.random.shuffle(indexes)\n",
    "    return indexes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the model\n",
    "def create_model(lstm_in_cols_obj, emb_in_cols_obj):\n",
    "    def create_emb(name, out_len, in_len):\n",
    "        inp = Input(shape=[1], dtype='int64', name=\"%s_in\" % name)\n",
    "        out = (Embedding(in_len, out_len, name=\"%s_emb\" % name)(inp))\n",
    "        out = SpatialDropout1D(0.2)(out)\n",
    "        out = Flatten()(out)\n",
    "        return inp,out\n",
    "\n",
    "\n",
    "    def create_lstm(name, lstm_out, timesteps_in, dims_in):\n",
    "        model_inp = Input(shape=(timesteps_in,dims_in), name=\"%s_in\" % name)\n",
    "        model_out = LSTM(lstm_out,return_sequences=False,name=\"%s_lstm\" % name)(model_inp)\n",
    "        return model_inp, model_out\n",
    "\n",
    "    lstm_in_array= []\n",
    "    lstm_out_array = []\n",
    "    for x in lstm_in_cols_obj:\n",
    "        name = x[\"name\"]\n",
    "        dims_in = len(x[\"cols\"])\n",
    "        lstm_out = x[\"out_l\"]\n",
    "        x_model_in, x_model_out = create_lstm(name=name, lstm_out=lstm_out, timesteps_in=71, dims_in=dims_in)\n",
    "        lstm_in_array.append(x_model_in)\n",
    "        lstm_out_array.append(x_model_out)\n",
    "    \n",
    "    #add first two\n",
    "    #model_lstm_add_two = add([lstm_out_array[0] , lstm_out_array[1]])\n",
    "    #model_lstm_add_two = PReLU()(model_lstm_add_two)\n",
    "    \n",
    "    \n",
    "    #add last three lstm\n",
    "    #model_lstm_add_three = add([lstm_out_array[2] , lstm_out_array[3] , lstm_out_array[4]])\n",
    "    #model_lstm_add_three = PReLU()(model_lstm_add_three)\n",
    "\n",
    "    #add last two lstm\n",
    "    model_lstm_add_two = add([lstm_out_array[1] , lstm_out_array[2]])\n",
    "    model_lstm_add_two = PReLU()(model_lstm_add_two)\n",
    "    \n",
    "    #concat with add layers + origin layer\n",
    "    model_lstm = concatenate([model_lstm_add_two] + lstm_out_array)\n",
    "    model_lstm = (Dense(200)(model_lstm))\n",
    "    model_lstm = PReLU()(model_lstm)\n",
    "\n",
    "    #create embeddings\n",
    "    in_array_emb= []\n",
    "    out_array_emb = []\n",
    "    for x in emb_in_cols_obj:  \n",
    "        in_len = x[\"in_l\"]\n",
    "        out_len = x[\"out_l\"]\n",
    "        name = x[\"name\"]\n",
    "        inp_emb, out_emb = create_emb(name=name , out_len=out_len, in_len = in_len)\n",
    "        in_array_emb.append(inp_emb)\n",
    "        out_array_emb.append((out_emb))\n",
    "        \n",
    "    model_emb = concatenate(out_array_emb) if len(out_array_emb) > 1 else out_array_emb[0]\n",
    "    model_emb = Dropout(0.2)(Dense(100)(model_emb))\n",
    "    model_emb = PReLU()(model_emb)\n",
    "    \n",
    "    #concatenate embeddings & lstms\n",
    "    model = concatenate([model_emb] + [model_lstm]+ [model_lstm_add_two])\n",
    "\n",
    "    model = Dense(300)(model)\n",
    "    model = PReLU()(model)\n",
    "    model = Dropout(0.2)(model)\n",
    "    model = Dense(1, activation=\"sigmoid\")(model)\n",
    "    model =  Model(lstm_in_array+in_array_emb, model)\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Params for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#params of the model\n",
    "lstm_in_cols_obj = [\n",
    "#{\"name\" : \"volatility\" , \"cols\": [volatility_cols] , \"out_l\": 45} ,\n",
    "{\"name\" : \"return\" , \"cols\": [return_cols] , \"out_l\": 30} , #checker pour le 45 devient 30\n",
    "{\"name\" : \"return_per_product\" , \"cols\": [return_avg_product_cols, return_median_product_cols, return_std_product_cols, return_nan_product_cols], \"out_l\": 15} ,\n",
    "{\"name\" : \"return_per_date\" , \"cols\": [return_avg_date_cols, return_median_date_cols, return_std_date_cols, return_nan_date_cols], \"out_l\": 15} , \n",
    "#{\"name\" : \"return_per_date_and_product\" , \"cols\": [return_avg_date_cols, return_kurt_date_cols , return_avg_product_cols, return_kurt_product_cols ], \"out_l\": 15}\n",
    "]\n",
    "\n",
    "emb_in_cols_obj = [\n",
    "    {\"name\" : \"emb_eqt_code\" , \"cols\": [[\"eqt_code\"]] , \"out_l\": 80, \"in_l\" : all_data[\"eqt_code\"].nunique()} ,\n",
    "    {\"name\" : \"countd_date\" , \"cols\": [[\"countd_date\"]] , \"out_l\": 10 , \"in_l\" : all_data[\"countd_date\"].nunique()} ,\n",
    "    {\"name\" : \"countd_product\" , \"cols\": [[\"countd_product\"]] , \"out_l\": 5 , \"in_l\" : all_data[\"countd_product\"].nunique()} ,\n",
    "    {\"name\" : \"countd_ret_nan\" , \"cols\": [[\"ret_nan\"]] , \"out_l\": 5 , \"in_l\" : all_data[\"ret_nan\"].nunique()}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "### add encoders in lstm_in_cols_obj \n",
    "lstm_in_cols_obj = fit_encoder(lstm_in_cols_obj, all_data , encoder_name=\"StandardScaler\")\n",
    "emb_in_cols_obj = fit_encoder(emb_in_cols_obj, all_data , encoder_name=\"LabelEncoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = all_data[all_data[\"is_train\"] == False]\n",
    "train_df = all_data[all_data[\"is_train\"] == True]\n",
    "#del all_data;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df_samp = train_df.sample(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = prepare_data(lstm_in_cols_obj, emb_in_cols_obj, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test = prepare_data(lstm_in_cols_obj, emb_in_cols_obj, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "emb_eqt_code_in (InputLayer)    (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "countd_date_in (InputLayer)     (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "countd_product_in (InputLayer)  (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "countd_ret_nan_in (InputLayer)  (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "emb_eqt_code_emb (Embedding)    (None, 1, 80)        54400       emb_eqt_code_in[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "countd_date_emb (Embedding)     (None, 1, 10)        3170        countd_date_in[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "countd_product_emb (Embedding)  (None, 1, 5)         165         countd_product_in[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "countd_ret_nan_emb (Embedding)  (None, 1, 5)         310         countd_ret_nan_in[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "return_per_product_in (InputLay (None, 71, 4)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "return_per_date_in (InputLayer) (None, 71, 4)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_1 (SpatialDro (None, 1, 80)        0           emb_eqt_code_emb[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_2 (SpatialDro (None, 1, 10)        0           countd_date_emb[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_3 (SpatialDro (None, 1, 5)         0           countd_product_emb[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_4 (SpatialDro (None, 1, 5)         0           countd_ret_nan_emb[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "return_per_product_lstm (LSTM)  (None, 15)           1200        return_per_product_in[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "return_per_date_lstm (LSTM)     (None, 15)           1200        return_per_date_in[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 80)           0           spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 10)           0           spatial_dropout1d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 5)            0           spatial_dropout1d_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 5)            0           spatial_dropout1d_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 15)           0           return_per_product_lstm[0][0]    \n",
      "                                                                 return_per_date_lstm[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "return_in (InputLayer)          (None, 71, 1)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 100)          0           flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "                                                                 flatten_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_1 (PReLU)               (None, 15)           15          add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "return_lstm (LSTM)              (None, 30)           3840        return_in[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 100)          10100       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 75)           0           p_re_lu_1[0][0]                  \n",
      "                                                                 return_lstm[0][0]                \n",
      "                                                                 return_per_product_lstm[0][0]    \n",
      "                                                                 return_per_date_lstm[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 100)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 200)          15200       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_3 (PReLU)               (None, 100)          100         dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_2 (PReLU)               (None, 200)          200         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 315)          0           p_re_lu_3[0][0]                  \n",
      "                                                                 p_re_lu_2[0][0]                  \n",
      "                                                                 p_re_lu_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 300)          94800       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_4 (PReLU)               (None, 300)          300         dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 300)          0           p_re_lu_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1)            301         dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 185,301\n",
      "Trainable params: 185,301\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model(lstm_in_cols_obj, emb_in_cols_obj)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model with 5 KFold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort values in another order\n",
    "#sort values per date in order to NOT train with dates (validation is done with dates never seen like in the test set) \n",
    "seed = 42\n",
    "#train_df[\"shuffle\"] = train_df['date'].apply(lambda x: hashlib.sha256(str(x*seed)).hexdigest())\n",
    "train_df.sort_values([\"date\"], inplace=True)\n",
    "monitor = \"val_loss\"\n",
    "model_prefix = \"cfm-example-model-%s\" % datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "total_folds = 5\n",
    "total_it = 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 596261 , Valid 149066\n",
      "Start Fold 1 - start base_lr 0.0010 : \n",
      "Epoch 1/50\n",
      "121/121 [==============================] - 856s 7s/step - loss: 0.6929 - acc: 0.5123 - val_loss: 0.6929 - val_acc: 0.5096\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.69292, saving model to weights/cfm-example-model-2019-02-28-best-fold-1-it-1.h5\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Unable to create file (unable to open file: name = 'weights/cfm-example-model-2019-02-28-best-fold-1-it-1.h5', errno = 2, error message = 'No such file or directory', flags = 13, o_flags = 602)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-6ca3b4bd0858>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m#from 8 to 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m                  )         \n\u001b[1;32m     42\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_checkpoint_name_tmp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/sgoinfre/goinfre/Perso/trmonich/miniconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/sgoinfre/goinfre/Perso/trmonich/miniconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n",
      "\u001b[0;32m/sgoinfre/goinfre/Perso/trmonich/miniconda3/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n",
      "\u001b[0;32m/sgoinfre/goinfre/Perso/trmonich/miniconda3/lib/python3.6/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n",
      "\u001b[0;32m/sgoinfre/goinfre/Perso/trmonich/miniconda3/lib/python3.6/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n",
      "\u001b[0;32m/sgoinfre/goinfre/Perso/trmonich/miniconda3/lib/python3.6/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36msave_weights\u001b[0;34m(self, filepath, overwrite)\u001b[0m\n",
      "\u001b[0;32m/sgoinfre/goinfre/Perso/trmonich/miniconda3/lib/python3.6/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, **kwds)\u001b[0m\n",
      "\u001b[0;32m/sgoinfre/goinfre/Perso/trmonich/miniconda3/lib/python3.6/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.create\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to create file (unable to open file: name = 'weights/cfm-example-model-2019-02-28-best-fold-1-it-1.h5', errno = 2, error message = 'No such file or directory', flags = 13, o_flags = 602)"
     ]
    }
   ],
   "source": [
    "kfold = KFold(n_splits=total_folds, shuffle=False)\n",
    "cvscores = []\n",
    "test_preds = []\n",
    "batch_size = (len(train_df) - len(train_df)//total_folds)//(total_it)\n",
    "i = 1\n",
    "for train, valid in kfold.split(train_df):\n",
    "    tmp_df_train = train_df.loc[train].reset_index(drop=True)\n",
    "    tmp_df_valid = train_df.loc[valid].sample(frac=1).reset_index(drop=True)\n",
    "    print(\"Train %d , Valid %d\" % (len(tmp_df_train) , len(tmp_df_valid)))\n",
    "\n",
    "    base_lr = 0.001\n",
    "    j = 1 \n",
    "    last_loss_score = 100 #default init.\n",
    "    model = create_model(lstm_in_cols_obj, emb_in_cols_obj)\n",
    "    once = False\n",
    "    print(\"Start Fold %i - start base_lr %0.4f : \" % (i , base_lr))\n",
    "    while True:\n",
    "        model_checkpoint_name_tmp = \"weights/%s-best-fold-%d-it-%d.h5\" % (model_prefix, i, j )\n",
    "        callbacks = [EarlyStopping(monitor=monitor, patience=3, verbose=1, min_delta=1e-8, mode='min'),\n",
    "                     ModelCheckpoint(model_checkpoint_name_tmp, monitor=monitor, save_best_only=True, save_weights_only=True, verbose=1, mode=\"min\")\n",
    "                    ]\n",
    "        step_size = 100\n",
    "        clr  = CyclicLR(mode='exp_range', max_lr=base_lr*6, base_lr=base_lr, step_size=step_size)\n",
    "        callbacks.append(clr)\n",
    "        \n",
    "        train_gen = generate(tmp_df_train, target_col, batch_size, lstm_in_cols_obj, emb_in_cols_obj, True, True)\n",
    "        valid_gen = generate(tmp_df_valid, target_col, batch_size, lstm_in_cols_obj, emb_in_cols_obj, True, False)\n",
    "        \n",
    "        model.compile(loss= \"binary_crossentropy\",                      \n",
    "                  optimizer='Adam',\n",
    "                      metrics=[\"accuracy\"])\n",
    "        hist = model.fit_generator(\n",
    "            generator=train_gen,\n",
    "            validation_data=valid_gen,\n",
    "            steps_per_epoch = math.ceil((tmp_df_train.shape[0])/batch_size),\n",
    "            validation_steps = math.ceil((tmp_df_valid.shape[0])/batch_size),\n",
    "            verbose=1,epochs=50,\n",
    "            callbacks = callbacks,\n",
    "            max_queue_size=30, workers=1, #from 8 to 1\n",
    "            use_multiprocessing=True,\n",
    "            workers=multiprocessing.cpu_count()-1\n",
    "                 )         \n",
    "        model.load_weights(model_checkpoint_name_tmp)\n",
    "        loss_score,score = model.evaluate_generator(valid_gen, steps=math.ceil((tmp_df_valid.shape[0])/batch_size))\n",
    "        print(\"Checkpoint. Fold %i | Iteration %i (LR %0.6f) -- Score --> %0.8f | %0.8f \"  % (i, j , base_lr , score,  loss_score))\n",
    "        if loss_score > last_loss_score:\n",
    "            print(\"Score Worse than before (%0.8f | %0.8f)\" % ( loss_score , last_loss_score))\n",
    "            if once:\n",
    "                break\n",
    "            once = True\n",
    "            print(\"Try one last time reducing LR.\")\n",
    "            base_lr = base_lr/10\n",
    "\n",
    "        else:\n",
    "            once = False\n",
    "            print(\"Score Better than before (%0.8f | %0.8f)\" % (loss_score ,last_loss_score))\n",
    "            last_loss_score=loss_score\n",
    "            base_lr = base_lr/5\n",
    "            best_j = j \n",
    "            print(\"Reducing LR to %f\" % base_lr)\n",
    "        if j >3:\n",
    "            print(\"Stop. Too Much Iterations.\")\n",
    "            break\n",
    "        model_checkpoint_name_tmp = \"weights/%s-best-fold-%d-it-%d.h5\" % (model_prefix, i, best_j )\n",
    "        model.load_weights(model_checkpoint_name_tmp)\n",
    "        j = j+1\n",
    "            \n",
    "\n",
    "    print(\"End of Training for Fold %i. Predicting.\"  % (i))\n",
    "    #GET LAST SCORE\n",
    "    model_checkpoint_name = \"weights/%s-best-fold-%d-it-%d.h5\" % (model_prefix, i, best_j )\n",
    "    model.load_weights(model_checkpoint_name)\n",
    "    #save best model\n",
    "    model.save_weights(\"weights/%s-best-fold-%d-Best.h5\" % (model_prefix, i) , overwrite=True )\n",
    "    score,score_mae = model.evaluate_generator(valid_gen, steps=math.ceil(len(tmp_df_valid)/batch_size))\n",
    "\n",
    "    test_yPreds = model.predict(X_test, batch_size=2048, verbose=0)\n",
    "    cvscores.append((score_mae,score))\n",
    "    test_preds.append(test_yPreds[:,-1])\n",
    "    \n",
    "    print(\"End of Fold %i | Score --> %0.4f | %0.4f\"  % (i, score, score_mae) )\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvscores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_cv_score = np.mean(cvscores, axis=0)\n",
    "mean_cv_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_test_score = np.mean(test_preds, axis=0)\n",
    "out_name = 'results/%s-%d-Kfolds_%0.6f_%0.6f.csv' % (model_prefix, total_folds, mean_cv_score[0], mean_cv_score[1])\n",
    "submission = pd.DataFrame({'ID':test_df[\"ID\"], 'TARGET':avg_test_score.ravel()})\n",
    "submission.to_csv(out_name , index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
